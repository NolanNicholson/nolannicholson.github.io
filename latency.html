<!DOCTYPE html>
<html>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-146424119-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-146424119-1');
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Negative Latency</title>
    <link rel="stylesheet" type="text/css" href="styles/highlighting.css">
    <link rel='stylesheet' type='text/css' href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,700'>
    <link rel="stylesheet" type="text/css" href="styles/main.css">
</head>

<body>
    <div id='name-banner'>
        <h1 class='name'>
            <a href="index.html">Nolan Nicholson</a>
            
            <!--Code for adding a spinning cube to the header-->
            <div id="headercanvas-holder">
                <canvas id="headercanvas"></canvas>
            </div>
            <script src="scripts/libraries/gl-matrix.js"></script>
            <script src="scripts/headercube.js"></script>
            <!--end spinning cube code-->
        </h1>
    </div>
    

<h2>Exploring "Negative Latency"</h2>

<p>
Google's new Stadia cloud gaming service launched
last Tuesday, and experiences with the platform have
been mixed. <a
    href="https://venturebeat.com/2019/11/18/google-stadia-review-it-works-but-so-what/">Some
    reviewers</a> have had it work almost flawlessly for them,
but still have some questions about the limited library and
somewhat unclear audience of the service.
<a
    href="https://www.youtube.com/watch?v=o6pf988yFSc">Others</a>
have been less lucky, experiencing comical amounts of latency
and other technical glitches.
</p>

<p>
People have already said a lot about Stadia's prospects, and
about those of cloud gaming in general, and
I won't try to retread that ground here.
Instead, I want to focus on a particular aspect of the technical
challenge facing Stadia -
one I think will be especially important to whether
cloud gaming lives or dies: network lag.
</p>

<p>
My pet theory is that people will tolerate lowish resolutions,
weird compression artifacts, mediocre framerates, and all
sorts of other issues, as long as their experience is snappy
and responsive.
But if it does <a
                  href="https://youtu.be/o6pf988yFSc?t=26">this</a>,
people will run away screaming. Even if each horribly-lagged
frame is rendered in full 4k60 beauty, it will not matter.
High latency can be such a dealbreaker that Nvidia's GeForce NOW
streaming service, a competitor to Stadia, <a
                                               href="https://shield.nvidia.com/support/geforce-now/system-requirements/2">will
   literally not let you stream with a connection speed under
   10 Mbps</a>.
</p>

<p>
So I was pleasantly surprised when Stadia's VP of
Engineering went out and said that Stadia would have <a
                                              href="https://www.engadget.com/2019/10/10/google-stadia-negative-latency/">
negative latency</a>:
</p>

<blockquote>
    "Negative latency" is a concept by which Stadia can set up
    a game with a buffer of predicted
    latency between the server and player, and then use
    various methods to undercut it. It can run the game at a
    super-fast framerate so it can act on player inputs
    earlier, or it can predict a player's button presses.
</blockquote>

<p>
Time-travel-sounding buzzword aside, I was intrigued
by the techniques listed here. Could Stadia really
predict inputs precisely enough, or run the game fast enough,
to effectively "negate" latency? If so, why were players
experiencing severe latency issues anyway?
To find out, I tried implementing some of these tricks myself.
</p>

<h3>First, a game</h3>
<p>
I thought the best way to try out some of these techniques
would be through experimenting on an actual game - something
simple, but with gameplay precise enough that any latency issues
would be nice and frustrating.
The result was, let's say, unoriginal.
I give you the new smash hit, "Flapping Bird":
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>

<p>
So we have a game to work with - but it is not representative of how
Stadia or other cloud gaming services work, for reasons we will soon see.
</p>

<h3>Simulating the Stadia situation</h3>

<p>
Like most other video games you have played before,
the above game is being run and rendered entirely on your own device.
This means that the latency is nice and low,
but it also means that I can only show you as sophisticated a
game as your device can actually run.
</p>

<p>
Cloud streaming flips this around by running the game on a computer
somewhere else. Your device records your inputs and sends them to the
game server, and the game server sends you back your video:
</p>

<img class="figure" src="images/latency/client_server.png"></img>

<p>
The benefit of this is that the game server
can be as powerful and well-maintained as Google's
money can buy. Your device just has to be powerful enough to stream
video, which most modern phones and TVs can do just fine. However, your
device and the game server now have to communicate with each other.
And that communication adds a delay.
</p>

<p>
This article won't attempt to actually deal with the video encoding
and decoding steps, which are a whole separate challenge in
their own right. Instead of sending encoded video data, we will
send a JSON string that describes each of the objects to render. 
The client will still handle the rendering. Also,
instead of actually sending that JSON over a real network, we will just
schedule in a delay before it "arrives." So our setup will be a little
bit different:
</p>

<img class="figure" src="images/latency/client_server_2.png"></img>

<!-- TODO: video proxy JSON and timeout code snippet -->

<p>
On the other side, we will program a similar delay into "sending" our
inputs. So there are two separate delays: one for our inputs to travel
to the server, and one for the video data to travel back.
Combine the two delays and, <i>voila</i>, we've taken a perfectly
good game and ruined it:
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
The slider above sets the total round-trip latency for the
game. This total latency is (arbitrarily) split evenly between your
inputs traveling to the "server" and the video data traveling back to
you. Try it out and see how added latency affects the game experience.
</p>

<p>
Now that we have latency in our system, we can start trying our
various tricks for undercutting it.
</p>

<h3>Trick 1: Run-Ahead</h3>
<p>
The first trick mentioned was to "run the game at a super-fast framerate
so it can act on player inputs earlier." Stadia would not be the first
system to use this sort of technique: in particular, a fellow named
Dwedit
<a
            href="https://www.libretro.com/index.php/retroarch-1-7-2%E2%80%8A-%E2%80%8Aachieving-better-latency-than-original-hardware-through-new-runahead-method/">
            implemented a method called Run-Ahead</a> for the
RetroArch retro game emulator, and people have speculated
that Stadia may be attempting something similar.
Run-Ahead basically basically works like this:
</p>

<ol>
    <li>Run the game one frame, but do not show any audio or video
        output.</li>
    <li>Save a state of the game at this point in time.</li>
    <li>Run the game as many more frames forward as needed to compensate
        for the game's lag. Again, do not show any audio or video.</li>
    <li>Show the audio and video for the latest frame.</li>
    <li>Load the previously saved game state, to "undo" the
        run-ahead.</li>
</ol>

<p>
So, if the game is expected to have 3 frames of lag, then it has to
run the game forward 3 "speculative" frames for every 1 frame
actually processed or displayed. In other words, the computer running
the game has to run it up to 3 times faster than it would normally
have to. (That said, actually rendering the video - often the most
compute-intensive step - only has to be done once, if your game engine
is set up the right way.)
</p>

<p>
Let's see what that extra processing gets us.
Here is a rough implementation of the run-ahead technique for our game:
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
My subjective experience is that this technique can sort of make
low latencies (say, up to 200 ms) feel even lower, but that
it causes unbearable jittering at higher latencies.
In particular, tapping the bird causes it to "snap" upward instead
of hopping smoothly.
</p>

<p>
Why does this happen?
There is one big difference between using Run-Ahead to manage the kind
of latency that it was developed for (i.e., RetroArch games), and
using it to manage our cloud gaming setup
In RetroArch, most of the lag happens <em>after</em>
inputs have already been received.
In our case, the inputs themselves have to travel to the server, so
there is also some latency in even receiving them.
That means a few (bad) things for Run-Ahead here:
</p>

<ol>
    <li>
        RetroArch can conceivably use Run-Ahead to get the observable
        latency down to less than 1 frame of gameplay. We can only get it
        down to however long it takes the inputs to travel to the
        server. For our simplified case, where the latency is split evenly
        between sending inputs and receiving the video, the best this
        technique can do is to cut the latency in half.
    </li>
    <li>
        Unlike the games for which Run-Ahead was designed, our server
        has 0 frames of intrinsic lag between receiving the input and
        seeing its effects. That means that, if we run 3 frames ahead,
        we will actually just skip the first 3 frames of our input
        actions - hence the bird "snapping" upward.
    </li>
</ol>

<p>
In other words, Run-Ahead alone won't solve our problem, since we
cannot wait for the inputs to travel to the server. What else can
we do?
</p>

<h3>Sending Speculative Video to the Client</h3>

<p>
So we know that we can only get our game video from the server, and we
know that we don't want to wait for our inputs to reach the server
first. How could we see the effects of our inputs without waiting for
the server? We could just have the server preemptively send us
video data for all possible cases, and have the client select the
appropriate one for the user's input.
</p>

<p>
For our little game, if there is only one frame of latency, you only
need two cases: a case for if the user clicks, and a case for if they
don't. If there are two frames of latency, you have to send four cases:
two cases for the current frame, times two cases for the previous one.
Extending this, we have to send 2<sup><i>N</i></sup> cases, where
<i>N</i> is the number of frames of lag. If you are playing a 60 fps
game with 200 ms latency, that means you will have about 12 frames of
latency, which will require that the server pre-emptively send you
4,096 possible cases to select from. And that is for a game with just
one binary input!
</p>

<p>
But all is not lost: we can simplify, at the cost of a slight loss in
accuracy. Instead of assuming the user can press the button every
single frame, let's assume they could only have pressed it once during
our lag period. That way, we only have to handle <i>N</i> cases: one
for if they didn't click recently, one for if they clicked at this
frame, one for if they clicked at previous frame, and so on.
</p>

<!--TODO: Discuss Id Software patent -->

<script src="scripts/latency_demo.js"></script>
<link rel="stylesheet" type="text/css" href="styles/latency_demo.css">


    
    <hr>
    <p>
        <a href="index.html">Home</a>
    </p>
    

</body>
</html>