{% extends "base.html" %}
{% block title %}Negative Latency{% endblock %}
{% block page_content %}

<h2>Exploring "Negative Latency"</h2>

<p>
Google's new Stadia cloud gaming service launched
last Tuesday, and experiences with the platform have
been mixed. <a
    href="https://venturebeat.com/2019/11/18/google-stadia-review-it-works-but-so-what/">Some
    reviewers</a> have had it work almost flawlessly for them,
but still have some questions about the limited library and
somewhat unclear audience of the service.
<a
    href="https://www.youtube.com/watch?v=o6pf988yFSc">Others</a>
have been less lucky, experiencing comical amounts of latency
and other technical glitches.
</p>

<p>
People online have already said a lot about Stadia's prospects, and
about those of cloud gaming in general, and
I won't try to retread that ground here.
Instead, I want to focus on a particular aspect of the technical
challenge facing Stadia -
one I think will be especially important to whether
cloud gaming lives or dies: network lag.
</p>

<p>
My pet theory is that people will tolerate lowish resolutions,
weird compression artifacts, mediocre framerates, and all
sorts of other issues, as long as their experience is snappy
and responsive.
But if it does <a
                  href="https://youtu.be/o6pf988yFSc?t=26">this</a>,
people will run away screaming. Even if each horribly-lagged
frame is rendered in full 4k60 beauty, it will not matter.
High latency can be such a dealbreaker that Nvidia's GeForce NOW
streaming service, a competitor to Stadia, <a
                                               href="https://shield.nvidia.com/support/geforce-now/system-requirements/2">will
   literally not let you stream with a connection speed under
   10 Mbps</a>.
</p>

<p>
So I was pleasantly surprised when Stadia's VP of
Engineering went out and said that Stadia would have <a
                                              href="https://www.engadget.com/2019/10/10/google-stadia-negative-latency/">
negative latency</a>:
</p>

<blockquote>
    "Negative latency" is a concept by which Stadia can set up
    a game with a buffer of predicted
    latency between the server and player, and then use
    various methods to undercut it. It can run the game at a
    super-fast framerate so it can act on player inputs
    earlier, or it can predict a player's button presses.
</blockquote>

<p>
Time-travel-sounding buzzword aside, I was intrigued
by the techniques listed here. Could Stadia really
predict inputs precisely enough, or run the game fast enough,
to effectively "negate" latency? If so, why were players
experiencing severe latency issues anyway?
To find out, I tried implementing some of these tricks myself.
</p>

<h3>First, a game</h3>
<p>
I thought the best way to try out some of these techniques
would be through experimenting on an actual game - something
simple, but with gameplay precise enough that any latency issues
would be nice and frustrating.
The result was, let's say, unoriginal.
I give you the new smash hit, "Flapping Bird":
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>

<p>
So we have a game to work with - but it is not representative of how
Stadia or other cloud gaming services work, for reasons we will soon see.
</p>

<h3>Simulating the Stadia situation</h3>

<p>
Like most other video games you have played before,
the above game is being run and rendered entirely on your own device.
This means that the latency is nice and low,
but it also means that I can only show you as sophisticated a
game as your device can actually run.
</p>

<p>
Cloud streaming flips this around by running the game on a computer
somewhere else. Your device records your inputs and sends them to the
game server, and the game server sends you back your video:
</p>

<p>
The benefit of this is that the game server
can be as powerful and well-maintained as Google's
money can buy; your device just has to be powerful enough to stream
video, which most modern phones and TVs already are. However, your
device and the game server now have to communicate with each other.
And that communication adds a delay.
</p>

<p>
This article won't attempt to actually deal with the video encoding
and decoding steps, which are actually a whole separate challenge in
their own right. Instead of sending encoded video data, we will
send a JSON string that describes all of the objects to render. And,
instead of actually sending that JSON over a network, we will just
program in a delay before it "arrives":
</p>

<!-- TODO: video proxy JSON and timeout code snippet -->

<p>
On the other side, we will program a similar delay into "sending" our
inputs:
</p>

<!-- TODO: event timeout code snippet -->

<p>
Combine the two delays and, <i>voila</i>, we've taken a perfectly
good game and ruined it.
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
The slider above can be used to set the total round-trip latency for the
game. This total latency is (arbitrarily) split evenly between your
inputs traveling to the "server" and its video data traveling back to
you. Try it out and see how added latency affects the experience.
</p>

<p>
Now that we have latency in our system, we can start trying various
tricks to under-cut it.
</p>

<h3>Trick 1: Run-Ahead</h3>
<p>
The first trick mentioned was to "run the game at a super-fast framerate
so it can act on player inputs earlier." Stadia would not be the first
system to use this sort of technique: in particular, a fellow named
Dwedit
<a
            href="https://www.libretro.com/index.php/retroarch-1-7-2%E2%80%8A-%E2%80%8Aachieving-better-latency-than-original-hardware-through-new-runahead-method/">
            implemented a method called Run-Ahead</a> for the
RetroArch retro game emulator, and people have speculated
that Stadia may be attempting something similar.
Run-Ahead basically basically works like this:
</p>

<ol>
    <li>Run the game one frame, but do not show any audio or video
        output.</li>
    <li>Save a state of the game at this point in time.</li>
    <li>Run the game as many more frames forward as needed to compensate
        for the game's lag. Again, do not show any audio or video.</li>
    <li>Show the audio and video for the latest frame.</li>
    <li>Load the previously saved game state, to "undo" the
        run-ahead.</li>
</ol>

<p>
So, if the game is expected to have 3 frames of lag, then it has to
run the game forward 3 "speculative" frames for every 1 frame
actually processed or displayed. In other words, the computer running
the game has to run it up to 3 times faster than it would normally
have to. (That said, actually rendering the video - often the most
compute-intensive step - only has to be done once, if your game engine
is set up the right way.)
</p>

<p>
Here is how the run-ahead technique performs for our game:
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<script src="scripts/latency_demo.js"></script>
<link rel="stylesheet" type="text/css" href="styles/latency_demo.css">

{% endblock %}
