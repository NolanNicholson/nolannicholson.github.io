{% extends "base.html" %}
{% block title %}Negative Latency{% endblock %}
{% block page_content %}

<h2>Exploring "Negative Latency"</h2>

<p>
Google's new Stadia cloud gaming service launched
recently, and experiences with the platform have
been mixed.
</p>

<p>
There has already been
<a href="https://twitter.com/mcclure111/status/1196557401710837762">a lot</a>
of good discussion about Stadia's prospects, and
about those of cloud gaming in general.
I won't try to retread that ground here.
Instead, I want to focus on a particular aspect of the technical
challenge facing Stadia -
one I think will be especially important to whether
cloud gaming lives or dies: network lag.
</p>

<p>
<a href="https://venturebeat.com/2019/11/18/google-stadia-review-it-works-but-so-what/">Some
    reviewers</a> have had it work almost flawlessly for them,
but still have some questions about the limited library and
somewhat unclear audience of the service.
<a
    href="https://www.youtube.com/watch?v=o6pf988yFSc">Others</a>
have been less lucky, experiencing comical amounts of latency
and other technical glitches.

<p>
My pet theory is that people will tolerate lowish resolutions,
weird compression artifacts, mediocre framerates, and all
sorts of other issues, as long as their experience is snappy
and responsive.
But if it does <a
                  href="https://youtu.be/o6pf988yFSc?t=26">this</a>,
people will run away screaming. Even if each horribly-lagged
frame is rendered in full 4k60 beauty, it will not matter.
High latency can be such a dealbreaker that Nvidia's GeForce NOW
streaming service, a competitor to Stadia, <a
                                               href="https://shield.nvidia.com/support/geforce-now/system-requirements/2">will
   literally not let you stream with a connection speed under
   10 Mbps</a>.
</p>

<p>
So I was pleasantly surprised when Stadia's VP of
Engineering went out and said that Stadia would have <a
                                              href="https://www.engadget.com/2019/10/10/google-stadia-negative-latency/">
negative latency</a>. Although the buzzword itself implies some kind
of physically impossible time travel,
the real explanation is more modest:
</p>

<blockquote>
    "Negative latency" is a concept by which Stadia can set up
    a game with a buffer of predicted
    latency between the server and player, and then use
    various methods to undercut it. It can run the game at a
    super-fast framerate so it can act on player inputs
    earlier, or it can predict a player's button presses.
</blockquote>

<p>
I was intrigued
by the techniques listed here. Could Stadia really
predict inputs precisely enough, or run the game fast enough,
to effectively "negate" latency? If so, why were players
experiencing severe latency issues anyway?
</p>

<p>
To find out, I tried implementing some of these tricks myself.
My results, at least so far, have been mixed - and point less to any
"solution" of the problem and more to a bunch of tricks that only
sort of work.
</p>

<h3>First, a game</h3>
<p>
I thought the best way to try out some of these techniques
would be through experimenting on an actual game - something
simple, but with gameplay precise enough that any latency issues
would be nice and frustrating. The result was... unoriginal.
I give you "Flapping Bird":
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>

<p>
We'll work with this game throughout the article - first by
"breaking" it through the introduction of some artificial network
latency, then by trying to "un-break" it using some of the techniques
Google and others have contemplated.
</p>

<h3>Simulating the Stadia situation</h3>

<p>
Like most other video games you have played before,
the above game is being run and rendered entirely on your own device.
This means that the latency is nice and low,
but it also means that I can only show you as sophisticated a
game as your device can actually run.
</p>

<p>
Cloud streaming flips this around by running the game on a computer
somewhere else. Your device records your inputs and sends them to the
game server, and the game server sends you back your video:
</p>

<img class="figure" src="images/latency/client_server.png"></img>

<p>
The benefit of this is that the game server
can be as powerful and well-maintained as Google's
money can buy. Your device just has to be powerful enough to stream
video, which most modern phones and TVs can do just fine. However, your
device and the game server now have to communicate with each other.
And that communication adds a delay.
</p>

<p>
This article won't attempt to actually deal with the video encoding
and decoding steps, which are a whole separate challenge in
their own right. Instead of sending encoded video data, we will
send a JSON string that describes each of the objects to render. 
The client will still handle the rendering. Also,
instead of actually sending that JSON over a real network, we will just
schedule in a delay before it "arrives." So our setup will be a little
bit different:
</p>

<img class="figure" src="images/latency/client_server_2.png"></img>

<p>
But the result will be that the "video" data generated by the game has
the same kind of latency in getting to your screen as we would expect
Stadia to have.
On the other side, we will program a similar delay into "sending" our
inputs. So there are two separate delays: one for our inputs to travel
to the server, and one for the video data to travel back.
</p>

<p>
Combine the two delays and, <i>voila</i>, we've taken a perfectly
good game and ruined it:
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
The slider above sets the total round-trip latency for the
game. This total latency is (arbitrarily) split evenly between your
inputs traveling to the "server" and the video data traveling back to
you. Try it out and see how added latency affects the game experience.
</p>

<p>
Now that we have latency in our system, we can start trying our
various tricks for undercutting it.
</p>

<h3>Trick 1: Run-Ahead</h3>
<p>
The first trick mentioned was to "run the game at a super-fast framerate
so it can act on player inputs earlier." Stadia would not be the first
system to use this sort of technique: in particular, a programmer named
Dwedit
<a
            href="https://www.libretro.com/index.php/retroarch-1-7-2%E2%80%8A-%E2%80%8Aachieving-better-latency-than-original-hardware-through-new-runahead-method/">
            implemented a method called Run-Ahead</a> for the
RetroArch retro game emulator, and people have speculated
that Stadia may be attempting something similar.
Run-Ahead basically basically works like this:
</p>

<ol>
    <li>Run the game one frame, but do not show any audio or video
        output.</li>
    <li>Save a state of the game at this point in time.</li>
    <li>Run the game as many more frames forward as needed to compensate
        for the game's lag. Again, do not show any audio or video.</li>
    <li>Show the audio and video for the latest frame.</li>
    <li>Load the previously saved game state, to "undo" the
        run-ahead.</li>
</ol>

<p>
So, if the game is expected to have 3 frames of lag, then it has to
run the game forward 3 "speculative" frames for every 1 frame
actually processed or displayed. In other words, the computer running
the game has to run it up to 3 times faster than it would normally
have to. (That said, actually rendering the video - often the most
compute-intensive step - only has to be done once, if your game engine
is set up the right way.)
</p>

<p>
Let's see what that extra processing gets us.
Here is a rough implementation of the run-ahead technique for our game:
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
My subjective experience is that this technique can sort of make
low latencies (say, up to 200 ms) feel even lower, but that
it causes unbearable jittering at higher latencies.
In particular, tapping the bird causes it to "snap" upward instead
of hopping smoothly.
</p>

<p>
Why do we get this upward "snap"?
There is one big difference between using Run-Ahead to manage the kind
of latency that it was developed for (i.e., RetroArch games), and
using it to manage our cloud gaming setup here.
In RetroArch, most of the lag happens <em>after</em>
the emulator has already received the inputs. The emulator is then
just waiting for the game to proceed - which is why it makes sense
to speed the game up.
In our case, the inputs themselves have to travel to the server, so
there is also some latency before the system receives them.
That means a few (bad) things for Run-Ahead here:
</p>

<ol>
    <li>
        RetroArch can conceivably use Run-Ahead to get the observable
        latency down to less than 1 frame of gameplay. We can only get it
        down to however long it takes the inputs to travel to the
        server. For our simplified case, where the latency is split evenly
        between sending inputs and receiving the video, the best this
        technique can do is to cut the latency in half.
    </li>
    <li>
        Unlike the games for which Run-Ahead was designed, our server
        has 0 frames of intrinsic lag between receiving the input and
        seeing its effects. That means that, if we run 3 frames ahead,
        we will actually just skip the first 3 frames of our input
        actions - hence the bird "snapping" upward.
    </li>
</ol>

<p>
In other words, Run-Ahead alone won't solve our problem, since it
was designed for a different kind of lag. What else can we do?
</p>

<h3>Sending Speculative Video to the Client</h3>

<p>
So we know that we can only get our game video from the server, and we
know that we don't want to wait for our inputs to reach the server
first. How could we see the effects of our inputs without waiting for
the server? We could just have the server preemptively send us
video data for a bunch of different cases, and have the client select the
appropriate one based on the user's input.
</p>

<p>
For our little game, if there is only one frame of latency, you only
need two cases: a case for if the user clicks, and a case for if they
don't. If there are two frames of latency, you have to send four cases:
two cases for the current frame, times two cases for the previous one.
Extending this, we have to send 2<sup><i>N</i></sup> cases, where
<i>N</i> is the number of frames of lag. If you are playing a 60 fps
game with 200 ms latency, that means you will have about 12 frames of
latency, which will require that the server pre-emptively send you
4,096 possible cases to select from. So you would need to do the
equivalent of streaming 4,000 HD videos at once!
</p>

<p>
But we can reduce that complexity, at the expense of some accuracy.
Instead of assuming the user can press the button every
single frame, let's assume they could only have pressed it once during
our lag period. That way, instead of 2<sup><i>N</i></sup> cases,
we will only have to handle <i>N</i> cases: one
for if they didn't click recently, one for if they clicked at this
frame, one for if they clicked at the previous frame, and so on.
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
This is an extremely rough proof of concept, and making it really
useful would take a lot more fine adjustments than I've made here,
but it shows that you can begin to compensate for
relatively high latencies with this technique. The downside, though,
is that even with our shortcut we are still transmitting a large
amount of "video" data: for 200 ms latency at 60 fps, we still need to
send 12 versions of each frame.
</p>

<h4>What about multiple inputs?</h4>

<p>
All of the above is for a very simple game with just one binary input:
either we are jumping, or we aren't. How would this work for a more
typical game, where the user has a whole controller full of different
inputs they can use?
</p>

<p>
id Software <a
    href="https://patents.google.com/patent/US10341678B2">has
    patented</a> a possible way to do it: rather than sending every
possible combination of all the different inputs, each of them gets
associated with a "motion vector" describing how the video feed would
change in response to that input. If a user has pressed multiple
inputs, then the client can just add up the motion vectors.
</p>

<p>
This could work - it seems especially well-suited to first-person 3D
games - but adding up the motion vectors to produce the correct video
is more work for the client (your phone or computer) to do, which
starts to erode at the advantages of cloud gaming.
</p>

<h3>Predicting User Inputs</h3>

<p>
Finally, Google could simply try to predict the user's button presses
and handle them beforehand. On one hand, if any group in the world has
the machine learning chops to predict exactly how people would play a
video game, it is probably Google. But on the other hand, this is a
uniquely punishing problem.
</p>

<p>
Imagine you have a self-driving car: you tell the computer a
destination, you flip the switch, and the
car's computer begins making (and executing on) predictions for how
you would drive. The car might occasionally behave differently than
you would, but disagreement is no problem as long as the car behaves
"normally" and ends up in the right place.
</p>

<p>
Now, imagine that you and the computer are both trying to drive at the
same time, and the car is constantly juggling between your inputs and
the computer's. Most of the time, you will both be doing the same
things. But occasionally, you and the computer will disagree on little
details, like
the exact timing and style of your movements. When that happens, the
car will likely get the computer's input first (computers are fast!)
and begin acting on it, but it will then switch a few milliseconds
later to your input (you're the one in charge!) The result: every
time you disagree, the car will jitter a little bit.
</p>

<p>
For a (horrible) example, here is a version where the server predicts
that if your last two hops were 300 ms apart, then you will hop again
300 ms later:
</p>

<div class="game-container">
    <canvas class="demo-canvas inactive-demo"></canvas>
    <div class="score-pane">
        <span class="score">0</span> üê•<br>
        <span class="hiscore">0</span> üëë
    </div>
</div>
<div class="latency-pane">
    <span class="latency-info">
        Added Latency:<br>
        <span class="latency-indicator"></span>
    </span>
    <span class="latency-input-container">
        <input type="range" min="0" max="500" value="200"
        class="latency-input">
    </span>
</div>

<p>
One other interesting option would be to combine input
prediction with the "speculative video" technique we explored above.
If you can predict that a
user's input is 99.999% likely to be one of 5 cases, then you only
have 5 possible frames of video to send over, drastically
reducing the amount of data you have to send. In the cases where the
inputs can't be predicted, or the prediction is wrong, you would have
some latency or jitter, but most of the time it would be great.
</p>

<h3>So now what?</h3>

<p>
We've seen a couple of techniques that people have speculated might be
used to fight latency in cloud gaming. They are not without their
drawbacks: some help manage the latency but add a lot of computing and
networking cost, and some create a gameplay experience even worse than the
latency itself was.
</p>

<p>
</p>

<script src="scripts/latency_demo.js"></script>
<link rel="stylesheet" type="text/css" href="styles/latency_demo.css">

{% endblock %}
